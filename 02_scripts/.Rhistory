# Load libraries (which have all the important functions)
pacman::p_load(tidyverse, lubridate)
# Ensure the computer knows where to look for your files
setwd(dirname(rstudioapi::documentPath()))
getwd
getwd()
# Load libraries (which have all the important functions)
pacman::p_load(tidyverse, lubridate)
# Ensure the computer knows where to look for your files
setwd(dirname(rstudioapi::documentPath()))
# This tells R to look for the folders and then giving them a name
data.dir <- "../01_data"
meta.dir <- "../00_meta"
# Do this for each dataset
file.name <- "durham_data_tutorial_version.csv"
file <- file.path(data.dir, file.name)
data <- read.csv(file)
View(data)
# Do this for each dataset
file.name <- "synoptic_data_tutorial_version.csv"
file <- file.path(data.dir, file.name)
durham_data <- read.csv(file)
# Check it out, see if there's any weird quirks that need to be cleaned!
View(durham_data)
# Do this for each dataset
file.name <- "synoptic_data_tutorial_version.csv"
file <- file.path(data.dir, file.name)
synoptic_data <- read.csv(file)
# Check it out, see if there's any weird quirks that need to be cleaned!
# load the metadurham_data separately using "readLines"
meta <- readLines( file, n=10) # metadurham_data stops on row 10
meta <- paste( gsub(',', '', meta), collapse="\n") #format better for viewing by removing errant commas
message(meta)
# now load the durham_data
durham_data <- read.csv(file, skip=12, row.names=NULL) #durham_data is on row 12+
head(durham_data)
# load the metadurham_data separately using "readLines"
meta <- readLines( file, n=10) # metadurham_data stops on row 10
meta <- paste( gsub(',', '', meta), collapse="\n") #format better for viewing by removing errant commas
message(meta)
# Load libraries (which have all the important functions).
# The first line of code only needs to be ran once! Run it by removing the "#" before clicking the play button
#install.packages("pacman")
pacman::p_load(tidyverse, lubridate)
# Ensure the computer knows where to look for your files
setwd(dirname(rstudioapi::documentPath()))
# This tells R to look for the folders and then giving them a name
data.dir <- "../01_data" # This folder has all the data
meta.dir <- "../00_meta" # This folder has all the meta data
# Do this for each dataset, to load it in
file.name <- "durham_data_tutorial_version.csv"
file <- file.path(data.dir, file.name)
durham_data <- read.csv(file)
head(durham_data)
# load the metadurham_data separately using "readLines"
meta <- readLines( file, n=10) # metadurham_data stops on row 10
meta <- paste( gsub(',', '', meta), collapse="\n") #format better for viewing by removing errant commas
message(meta)
# now load the durham_data
durham_data <- read.csv(file, skip=12, row.names=NULL) #durham_data is on row 12+
head(durham_data)
# load the metadurham_data separately using "readLines"
meta <- readLines( file, n=10) # metadurham_data stops on row 10
meta <- paste( gsub(',', '', meta), collapse="\n") #format better for viewing by removing errant commas
view(meta)
# now load the durham_data
durham_data <- read.csv(file, skip=12, row.names=NULL) #durham_data is on row 12+
head(durham_data)
print(meta)
# load the metadurham_data separately using "readLines"
meta <- readLines( file, n=10) # metadurham_data stops on row 10
meta <- paste( gsub(',', '', meta), collapse="\n") #format better for viewing by removing errant commas
print(meta)
# now load the durham_data
durham_data <- read.csv(file, skip=12, row.names=NULL) #durham_data is on row 12+
head(durham_data)
print(durham_data)
View(durham_data)
print(durham_data)
head(durham_data)
# Load libraries (which have all the important functions).
# The first line of code only needs to be ran once! Run it by removing the "#" before clicking the play button
#install.packages("pacman")
pacman::p_load(tidyverse, lubridate)
# Ensure the computer knows where to look for your files
setwd(dirname(rstudioapi::documentPath()))
# This tells R to look for the folders and then giving them a name
data.dir <- "../01_data" # This folder has all the data
meta.dir <- "../00_meta" # This folder has all the meta data
# Do this for each dataset, to load it in
file.name <- "durham_data_tutorial_version.csv"
file <- file.path(data.dir, file.name)
durham_data <- read.csv(file)
print(durham_data)
head(durham_data)
durham_data %>%
select(Sky.Condition) %>%
unique() %>%
sort()
durham_data %>%
dplyr::select(Sky.Condition) %>%
unique() %>%
sort()
# Load libraries (which have all the important functions).
# The first line of code only needs to be ran once! Run it by removing the "#" before clicking the play button
#install.packages("pacman")
pacman::p_load(tidyverse, lubridate)
# Ensure the computer knows where to look for your files
setwd(dirname(rstudioapi::documentPath()))
# This tells R to look for the folders and then giving them a name
data.dir <- "../01_data" # This folder has all the data
meta.dir <- "../00_meta" # This folder has all the meta data
# Do this for each dataset, to load it in
file.name <- "durham_data_tutorial_version.csv"
file <- file.path(data.dir, file.name)
durham_data <- read.csv(file)
head(durham_data)
# load the metadurham_data separately using "readLines"
meta <- readLines( file, n=10) # metadurham_data stops on row 10
meta <- paste( gsub(',', '', meta), collapse="\n") #format better for viewing by removing errant commas
print(meta)
# now load the durham_data
durham_data <- read.csv(file, skip=12, row.names=NULL) #durham_data is on row 12+
head(durham_data)
meta.file.name <- 'metadurham_data.txt'
meta.file <- file.path(meta.dir, file.name)
write.table(meta, file=meta.file, row.names=FALSE, col.names=FALSE)
durham_data %>%
dplyr::select(Sky.Condition) %>%
unique() %>%
sort()
View(durham_data)
durham_data %>%
dplyr::select(Sky.Condition) #%>%
durham_data %>%
dplyr::select(Sky.Condition) %>%
unique(Sky.Condition) %>%
sort()
sky_condition<-durham_data %>%
dplyr::select(Sky.Condition) %>%
dplyr::distinct() %>%
sort()
sky_condition<-durham_data %>%
dplyr::select(Sky.Condition) %>%
dplyr::distinct(.keep_all = TRUE) %>%
sort()
colnames(durham_data)
sky_condition<-durham_data %>%
dplyr::select(Sky.Condition) %>%
dplyr::distinct(.keep_all = TRUE) %>%
sort()
sky_condition<-durham_data %>%
dplyr::select(Sky.Condition) #%>%
View(sky_condition)
sky_condition<-durham_data %>%
dplyr::select(Sky.Condition) %>%
count()
sky_condition<-durham_data %>%
dplyr::select(Sky.Condition) %>%
count()
view(sky_condition)
#sort( unique( durham_data$Sky.Condition ) )
sky_condition<-durham_data %>%
group_by(Sky.Condition) %>%
count()
view(sky_condition)
sky_condition<-durham_data %>%
group_by(Sky.Condition) %>%
count()
print(sky_condition)
#sort( unique( durham_data$Sky.Condition ) )
sky_condition<-durham_data %>%
group_by(Sky.Condition) %>%
count() %>%
arrange(desc(n))
print(sky_condition)
# Load libraries (which have all the important functions).
# The first line of code only needs to be ran once! Run it by removing the "#" before clicking the play button
#install.packages("pacman")
pacman::p_load(tidyverse, lubridate)
# Ensure the computer knows where to look for your files
setwd(dirname(rstudioapi::documentPath()))
# This tells R to look for the folders and then giving them a name
data.dir <- "../01_data" # This folder has all the data
meta.dir <- "../00_meta" # This folder has all the meta data
# Do this for each dataset, to load it in
file.name <- "durham_data_tutorial_version.csv"
file <- file.path(data.dir, file.name)
durham_data <- read.csv(file)
print(durham_data)
head(durham_data)
print(durham_data)
head(durham_data)
